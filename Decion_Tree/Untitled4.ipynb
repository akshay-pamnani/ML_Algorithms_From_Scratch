{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dbdf0e5-8840-4825-b24b-f40df86d432a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.12, pytest-7.1.1, pluggy-1.0.0\n",
      "rootdir: /Users/akshaypamnani/Desktop/USF-MSDS 2022_Academics/Machine Learning/ML_HW3\n",
      "plugins: anyio-3.5.0\n",
      "collected 7 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "test_dtree.py \u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                    [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________________________________ test_boston __________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_boston\u001b[39;49;00m():\n",
      "        X, y = load_boston(return_X_y=\u001b[94mTrue\u001b[39;49;00m)\n",
      ">       run_regression_test(X, y, ntrials=\u001b[94m10\u001b[39;49;00m, grace=\u001b[94m0.11\u001b[39;49;00m)\n",
      "\n",
      "\u001b[1m\u001b[31mtest_dtree.py\u001b[0m:10: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtest_dtree.py\u001b[0m:50: in run_regression_test\n",
      "    dt.fit(X_train, y_train)\n",
      "\u001b[1m\u001b[31mdtree.py\u001b[0m:171: in fit\n",
      "    \u001b[96mself\u001b[39;49;00m.root = \u001b[96mself\u001b[39;49;00m.fit_(X, y)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <dtree.RegressionTree621 object at 0x7fa6d213f700>\n",
      "X = array([[1.9650e-02, 8.0000e+01, 1.7600e+00, ..., 1.8200e+01, 3.4160e+02,\n",
      "        1.2930e+01],\n",
      "       [2.1161e-01, 0.00...02,\n",
      "        1.0360e+01],\n",
      "       [8.2440e-02, 3.0000e+01, 4.9300e+00, ..., 1.6600e+01, 3.7941e+02,\n",
      "        6.3600e+00]])\n",
      "y = array([20.1, 19.3, 20.3, 17.8, 19.9, 20.9, 14.1, 18.3, 14.9, 13.4, 20.1,\n",
      "        8.4, 22.8,  8.3, 24.3, 20. , 13.3, 18...    23.9, 14.8, 11.7, 24.3, 14.8, 42.3, 29.6, 18.4, 19.3, 14.1, 13.2,\n",
      "       24.3, 10.4, 33.4, 16.2, 27.5, 23.1, 23.7])\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mfit_\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, X, y):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Recursively create and return a decision tree fit to (X,y) for\u001b[39;49;00m\n",
      "    \u001b[33m    either a classification or regression.  This function should call self.create_leaf(X,y)\u001b[39;49;00m\n",
      "    \u001b[33m    to create the appropriate leaf node, which will invoke either\u001b[39;49;00m\n",
      "    \u001b[33m    RegressionTree621.create_leaf() or ClassifierTree621.create_leaf() depending\u001b[39;49;00m\n",
      "    \u001b[33m    on the type of self.\u001b[39;49;00m\n",
      "    \u001b[33m    This function is not part of the class \"interface\" and is for internal use, but it\u001b[39;49;00m\n",
      "    \u001b[33m    embodies the decision tree fitting algorithm.\u001b[39;49;00m\n",
      "    \u001b[33m    (Make sure to call fit_() not fit() recursively.)\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "        ...\n",
      "        \u001b[94mif\u001b[39;49;00m np.shape(X)[\u001b[94m0\u001b[39;49;00m] <= \u001b[96mself\u001b[39;49;00m.min_samples_leaf \u001b[95mor\u001b[39;49;00m np.shape(np.unique(X, axis=\u001b[94m0\u001b[39;49;00m)[\u001b[94m0\u001b[39;49;00m])==\u001b[94m1\u001b[39;49;00m:\n",
      "            \u001b[94mreturn\u001b[39;49;00m create_leaf(y)\n",
      "    \n",
      "        col,split=find_best_split(X,y,\u001b[96mself\u001b[39;49;00m.loss,\u001b[96mself\u001b[39;49;00m.min_samples_leaf)\n",
      "    \n",
      "        \u001b[94mif\u001b[39;49;00m col==-\u001b[94m1\u001b[39;49;00m:\n",
      "            \u001b[94mreturn\u001b[39;49;00m create_leaf(y)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "        \u001b[90m#node=DecisionNode(col,split,fit_(self,X,y)\u001b[39;49;00m\n",
      "            y_reshape=np.reshape(y,(np.shape(y)[\u001b[94m0\u001b[39;49;00m],\u001b[94m1\u001b[39;49;00m)) \u001b[90m#reshaping so that dimensions align\u001b[39;49;00m\n",
      "            Xy=np.hstack((X,y_reshape))\n",
      "    \n",
      "    \n",
      "    \n",
      ">           lchild=fit_(Xy[Xy[col]<split][:,:-\u001b[94m1\u001b[39;49;00m],Xy[Xy[col]<split][:,-\u001b[94m1\u001b[39;49;00m])\n",
      "\u001b[1m\u001b[31mE           NameError: name 'fit_' is not defined\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mdtree.py\u001b[0m:200: NameError\n",
      "\u001b[31m\u001b[1m_________________________ test_boston_min_samples_leaf _________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_boston_min_samples_leaf\u001b[39;49;00m():\n",
      "        X, y = load_boston(return_X_y=\u001b[94mTrue\u001b[39;49;00m)\n",
      ">       run_regression_test(X, y, ntrials=\u001b[94m10\u001b[39;49;00m, min_samples_leaf=\u001b[94m3\u001b[39;49;00m, grace=\u001b[94m0.09\u001b[39;49;00m, training_accuracy=\u001b[94m.94\u001b[39;49;00m)\n",
      "\n",
      "\u001b[1m\u001b[31mtest_dtree.py\u001b[0m:14: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtest_dtree.py\u001b[0m:50: in run_regression_test\n",
      "    dt.fit(X_train, y_train)\n",
      "\u001b[1m\u001b[31mdtree.py\u001b[0m:171: in fit\n",
      "    \u001b[96mself\u001b[39;49;00m.root = \u001b[96mself\u001b[39;49;00m.fit_(X, y)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <dtree.RegressionTree621 object at 0x7fa6d2125340>\n",
      "X = array([[6.27390e-01, 0.00000e+00, 8.14000e+00, ..., 2.10000e+01,\n",
      "        3.95620e+02, 8.47000e+00],\n",
      "       [6.65492e+0...+02, 1.29300e+01],\n",
      "       [1.18123e+01, 0.00000e+00, 1.81000e+01, ..., 2.02000e+01,\n",
      "        4.84500e+01, 2.27400e+01]])\n",
      "y = array([19.9, 19.5, 13.4, 43.5, 50. , 33.8, 17.5, 50. , 19.7, 13.1, 20.7,\n",
      "       13.5,  8.1, 22.9, 17.5, 10.5, 18.2, 13...    24.5, 18.4, 15.6, 23.1,  6.3, 14.6, 14.3, 22.1, 23. , 50. , 13.6,\n",
      "       17.6, 13.8, 15. , 19. , 19.9, 20.1,  8.4])\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mfit_\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, X, y):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Recursively create and return a decision tree fit to (X,y) for\u001b[39;49;00m\n",
      "    \u001b[33m    either a classification or regression.  This function should call self.create_leaf(X,y)\u001b[39;49;00m\n",
      "    \u001b[33m    to create the appropriate leaf node, which will invoke either\u001b[39;49;00m\n",
      "    \u001b[33m    RegressionTree621.create_leaf() or ClassifierTree621.create_leaf() depending\u001b[39;49;00m\n",
      "    \u001b[33m    on the type of self.\u001b[39;49;00m\n",
      "    \u001b[33m    This function is not part of the class \"interface\" and is for internal use, but it\u001b[39;49;00m\n",
      "    \u001b[33m    embodies the decision tree fitting algorithm.\u001b[39;49;00m\n",
      "    \u001b[33m    (Make sure to call fit_() not fit() recursively.)\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "        ...\n",
      "        \u001b[94mif\u001b[39;49;00m np.shape(X)[\u001b[94m0\u001b[39;49;00m] <= \u001b[96mself\u001b[39;49;00m.min_samples_leaf \u001b[95mor\u001b[39;49;00m np.shape(np.unique(X, axis=\u001b[94m0\u001b[39;49;00m)[\u001b[94m0\u001b[39;49;00m])==\u001b[94m1\u001b[39;49;00m:\n",
      "            \u001b[94mreturn\u001b[39;49;00m create_leaf(y)\n",
      "    \n",
      "        col,split=find_best_split(X,y,\u001b[96mself\u001b[39;49;00m.loss,\u001b[96mself\u001b[39;49;00m.min_samples_leaf)\n",
      "    \n",
      "        \u001b[94mif\u001b[39;49;00m col==-\u001b[94m1\u001b[39;49;00m:\n",
      "            \u001b[94mreturn\u001b[39;49;00m create_leaf(y)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "        \u001b[90m#node=DecisionNode(col,split,fit_(self,X,y)\u001b[39;49;00m\n",
      "            y_reshape=np.reshape(y,(np.shape(y)[\u001b[94m0\u001b[39;49;00m],\u001b[94m1\u001b[39;49;00m)) \u001b[90m#reshaping so that dimensions align\u001b[39;49;00m\n",
      "            Xy=np.hstack((X,y_reshape))\n",
      "    \n",
      "    \n",
      "    \n",
      ">           lchild=fit_(Xy[Xy[col]<split][:,:-\u001b[94m1\u001b[39;49;00m],Xy[Xy[col]<split][:,-\u001b[94m1\u001b[39;49;00m])\n",
      "\u001b[1m\u001b[31mE           NameError: name 'fit_' is not defined\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mdtree.py\u001b[0m:200: NameError\n",
      "\u001b[31m\u001b[1m___________________________ test_california_housing ____________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_california_housing\u001b[39;49;00m():\n",
      "        X, y = fetch_california_housing(return_X_y=\u001b[94mTrue\u001b[39;49;00m)\n",
      ">       run_regression_test(X, y, ntrials=\u001b[94m10\u001b[39;49;00m, grace=\u001b[94m0.19\u001b[39;49;00m)\n",
      "\n",
      "\u001b[1m\u001b[31mtest_dtree.py\u001b[0m:18: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtest_dtree.py\u001b[0m:50: in run_regression_test\n",
      "    dt.fit(X_train, y_train)\n",
      "\u001b[1m\u001b[31mdtree.py\u001b[0m:171: in fit\n",
      "    \u001b[96mself\u001b[39;49;00m.root = \u001b[96mself\u001b[39;49;00m.fit_(X, y)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <dtree.RegressionTree621 object at 0x7fa6e090c9d0>\n",
      "X = array([[   2.4875    ,   26.        ,    4.55647668, ...,    2.63108808,\n",
      "          38.09      , -122.23      ],\n",
      "      ...    ],\n",
      "       [   2.9817    ,   21.        ,    4.87698834, ...,    2.92258749,\n",
      "          37.44      , -122.07      ]])\n",
      "y = array([0.9    , 3.465  , 2.164  , 2.477  , 0.612  , 0.582  , 1.694  ,\n",
      "       4.399  , 1.646  , 0.498  , 1.873  , 0.858...    1.213  , 2.25   , 3.187  , 2.14   , 0.425  , 1.458  , 1.055  ,\n",
      "       0.834  , 1.727  , 5.00001, 0.723  , 2.25   ])\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mfit_\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, X, y):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Recursively create and return a decision tree fit to (X,y) for\u001b[39;49;00m\n",
      "    \u001b[33m    either a classification or regression.  This function should call self.create_leaf(X,y)\u001b[39;49;00m\n",
      "    \u001b[33m    to create the appropriate leaf node, which will invoke either\u001b[39;49;00m\n",
      "    \u001b[33m    RegressionTree621.create_leaf() or ClassifierTree621.create_leaf() depending\u001b[39;49;00m\n",
      "    \u001b[33m    on the type of self.\u001b[39;49;00m\n",
      "    \u001b[33m    This function is not part of the class \"interface\" and is for internal use, but it\u001b[39;49;00m\n",
      "    \u001b[33m    embodies the decision tree fitting algorithm.\u001b[39;49;00m\n",
      "    \u001b[33m    (Make sure to call fit_() not fit() recursively.)\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "        ...\n",
      "        \u001b[94mif\u001b[39;49;00m np.shape(X)[\u001b[94m0\u001b[39;49;00m] <= \u001b[96mself\u001b[39;49;00m.min_samples_leaf \u001b[95mor\u001b[39;49;00m np.shape(np.unique(X, axis=\u001b[94m0\u001b[39;49;00m)[\u001b[94m0\u001b[39;49;00m])==\u001b[94m1\u001b[39;49;00m:\n",
      "            \u001b[94mreturn\u001b[39;49;00m create_leaf(y)\n",
      "    \n",
      "        col,split=find_best_split(X,y,\u001b[96mself\u001b[39;49;00m.loss,\u001b[96mself\u001b[39;49;00m.min_samples_leaf)\n",
      "    \n",
      "        \u001b[94mif\u001b[39;49;00m col==-\u001b[94m1\u001b[39;49;00m:\n",
      "            \u001b[94mreturn\u001b[39;49;00m create_leaf(y)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "        \u001b[90m#node=DecisionNode(col,split,fit_(self,X,y)\u001b[39;49;00m\n",
      "            y_reshape=np.reshape(y,(np.shape(y)[\u001b[94m0\u001b[39;49;00m],\u001b[94m1\u001b[39;49;00m)) \u001b[90m#reshaping so that dimensions align\u001b[39;49;00m\n",
      "            Xy=np.hstack((X,y_reshape))\n",
      "    \n",
      "    \n",
      "    \n",
      ">           lchild=fit_(Xy[Xy[col]<split][:,:-\u001b[94m1\u001b[39;49;00m],Xy[Xy[col]<split][:,-\u001b[94m1\u001b[39;49;00m])\n",
      "\u001b[1m\u001b[31mE           NameError: name 'fit_' is not defined\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mdtree.py\u001b[0m:200: NameError\n",
      "\u001b[31m\u001b[1m__________________________________ test_iris ___________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_iris\u001b[39;49;00m():\n",
      "        X, y = load_iris(return_X_y=\u001b[94mTrue\u001b[39;49;00m)\n",
      ">       run_classification_test(X, y, ntrials=\u001b[94m10\u001b[39;49;00m, grace=\u001b[94m0.06\u001b[39;49;00m)\n",
      "\n",
      "\u001b[1m\u001b[31mtest_dtree.py\u001b[0m:22: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtest_dtree.py\u001b[0m:87: in run_classification_test\n",
      "    dt.fit(X_train, y_train)\n",
      "\u001b[1m\u001b[31mdtree.py\u001b[0m:171: in fit\n",
      "    \u001b[96mself\u001b[39;49;00m.root = \u001b[96mself\u001b[39;49;00m.fit_(X, y)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <dtree.ClassifierTree621 object at 0x7fa6e091be80>\n",
      "X = array([[7.6, 3. , 6.6, 2.1],\n",
      "       [6.3, 2.5, 5. , 1.9],\n",
      "       [6.5, 3. , 5.2, 2. ],\n",
      "       [5. , 3.2, 1.2, 0.2],\n",
      "  ...],\n",
      "       [4.6, 3.4, 1.4, 0.3],\n",
      "       [6.1, 2.6, 5.6, 1.4],\n",
      "       [6.3, 2.8, 5.1, 1.5],\n",
      "       [5.6, 2.5, 3.9, 1.1]])\n",
      "y = array([2, 2, 2, 0, 1, 0, 2, 0, 1, 0, 2, 2, 2, 1, 0, 1, 1, 0, 2, 2, 0, 2,\n",
      "       2, 1, 2, 2, 1, 1, 0, 2, 1, 0, 1, 1, 2,...2, 1, 1,\n",
      "       0, 1, 2, 0, 2, 1, 1, 1, 0, 0, 0, 2, 2, 0, 2, 2, 1, 0, 2, 2, 0, 0,\n",
      "       1, 0, 1, 0, 2, 0, 0, 2, 2, 1])\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mfit_\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, X, y):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Recursively create and return a decision tree fit to (X,y) for\u001b[39;49;00m\n",
      "    \u001b[33m    either a classification or regression.  This function should call self.create_leaf(X,y)\u001b[39;49;00m\n",
      "    \u001b[33m    to create the appropriate leaf node, which will invoke either\u001b[39;49;00m\n",
      "    \u001b[33m    RegressionTree621.create_leaf() or ClassifierTree621.create_leaf() depending\u001b[39;49;00m\n",
      "    \u001b[33m    on the type of self.\u001b[39;49;00m\n",
      "    \u001b[33m    This function is not part of the class \"interface\" and is for internal use, but it\u001b[39;49;00m\n",
      "    \u001b[33m    embodies the decision tree fitting algorithm.\u001b[39;49;00m\n",
      "    \u001b[33m    (Make sure to call fit_() not fit() recursively.)\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "        ...\n",
      "        \u001b[94mif\u001b[39;49;00m np.shape(X)[\u001b[94m0\u001b[39;49;00m] <= \u001b[96mself\u001b[39;49;00m.min_samples_leaf \u001b[95mor\u001b[39;49;00m np.shape(np.unique(X, axis=\u001b[94m0\u001b[39;49;00m)[\u001b[94m0\u001b[39;49;00m])==\u001b[94m1\u001b[39;49;00m:\n",
      "            \u001b[94mreturn\u001b[39;49;00m create_leaf(y)\n",
      "    \n",
      "        col,split=find_best_split(X,y,\u001b[96mself\u001b[39;49;00m.loss,\u001b[96mself\u001b[39;49;00m.min_samples_leaf)\n",
      "    \n",
      "        \u001b[94mif\u001b[39;49;00m col==-\u001b[94m1\u001b[39;49;00m:\n",
      "            \u001b[94mreturn\u001b[39;49;00m create_leaf(y)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "        \u001b[90m#node=DecisionNode(col,split,fit_(self,X,y)\u001b[39;49;00m\n",
      "            y_reshape=np.reshape(y,(np.shape(y)[\u001b[94m0\u001b[39;49;00m],\u001b[94m1\u001b[39;49;00m)) \u001b[90m#reshaping so that dimensions align\u001b[39;49;00m\n",
      "            Xy=np.hstack((X,y_reshape))\n",
      "    \n",
      "    \n",
      "    \n",
      ">           lchild=fit_(Xy[Xy[col]<split][:,:-\u001b[94m1\u001b[39;49;00m],Xy[Xy[col]<split][:,-\u001b[94m1\u001b[39;49;00m])\n",
      "\u001b[1m\u001b[31mE           NameError: name 'fit_' is not defined\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mdtree.py\u001b[0m:200: NameError\n",
      "\u001b[31m\u001b[1m__________________________________ test_wine ___________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_wine\u001b[39;49;00m():\n",
      "        X, y = load_wine(return_X_y=\u001b[94mTrue\u001b[39;49;00m)\n",
      ">       run_classification_test(X, y, ntrials=\u001b[94m10\u001b[39;49;00m, grace=\u001b[94m0.05\u001b[39;49;00m)\n",
      "\n",
      "\u001b[1m\u001b[31mtest_dtree.py\u001b[0m:26: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtest_dtree.py\u001b[0m:87: in run_classification_test\n",
      "    dt.fit(X_train, y_train)\n",
      "\u001b[1m\u001b[31mdtree.py\u001b[0m:171: in fit\n",
      "    \u001b[96mself\u001b[39;49;00m.root = \u001b[96mself\u001b[39;49;00m.fit_(X, y)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <dtree.ClassifierTree621 object at 0x7fa6e090aa30>\n",
      "X = array([[ 11.81,   2.12,   2.74, ...,   0.95,   2.26, 625.  ],\n",
      "       [ 12.29,   3.17,   2.21, ...,   1.42,   2.83, 406...  [ 12.86,   1.35,   2.32, ...,   0.76,   1.29, 630.  ],\n",
      "       [ 12.51,   1.24,   2.25, ...,   0.75,   1.51, 650.  ]])\n",
      "y = array([1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 0, 0, 0,\n",
      "       1, 2, 2, 1, 0, 2, 1, 1, 2, 1, 1, 2, 1,...2, 0, 1,\n",
      "       0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 1, 2, 0, 1, 1, 0, 0, 1, 1, 1, 2, 0,\n",
      "       2, 1, 0, 0, 2, 1, 2, 2, 2, 2])\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mfit_\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, X, y):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Recursively create and return a decision tree fit to (X,y) for\u001b[39;49;00m\n",
      "    \u001b[33m    either a classification or regression.  This function should call self.create_leaf(X,y)\u001b[39;49;00m\n",
      "    \u001b[33m    to create the appropriate leaf node, which will invoke either\u001b[39;49;00m\n",
      "    \u001b[33m    RegressionTree621.create_leaf() or ClassifierTree621.create_leaf() depending\u001b[39;49;00m\n",
      "    \u001b[33m    on the type of self.\u001b[39;49;00m\n",
      "    \u001b[33m    This function is not part of the class \"interface\" and is for internal use, but it\u001b[39;49;00m\n",
      "    \u001b[33m    embodies the decision tree fitting algorithm.\u001b[39;49;00m\n",
      "    \u001b[33m    (Make sure to call fit_() not fit() recursively.)\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "        ...\n",
      "        \u001b[94mif\u001b[39;49;00m np.shape(X)[\u001b[94m0\u001b[39;49;00m] <= \u001b[96mself\u001b[39;49;00m.min_samples_leaf \u001b[95mor\u001b[39;49;00m np.shape(np.unique(X, axis=\u001b[94m0\u001b[39;49;00m)[\u001b[94m0\u001b[39;49;00m])==\u001b[94m1\u001b[39;49;00m:\n",
      "            \u001b[94mreturn\u001b[39;49;00m create_leaf(y)\n",
      "    \n",
      "        col,split=find_best_split(X,y,\u001b[96mself\u001b[39;49;00m.loss,\u001b[96mself\u001b[39;49;00m.min_samples_leaf)\n",
      "    \n",
      "        \u001b[94mif\u001b[39;49;00m col==-\u001b[94m1\u001b[39;49;00m:\n",
      "            \u001b[94mreturn\u001b[39;49;00m create_leaf(y)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "        \u001b[90m#node=DecisionNode(col,split,fit_(self,X,y)\u001b[39;49;00m\n",
      "            y_reshape=np.reshape(y,(np.shape(y)[\u001b[94m0\u001b[39;49;00m],\u001b[94m1\u001b[39;49;00m)) \u001b[90m#reshaping so that dimensions align\u001b[39;49;00m\n",
      "            Xy=np.hstack((X,y_reshape))\n",
      "    \n",
      "    \n",
      "    \n",
      ">           lchild=fit_(Xy[Xy[col]<split][:,:-\u001b[94m1\u001b[39;49;00m],Xy[Xy[col]<split][:,-\u001b[94m1\u001b[39;49;00m])\n",
      "\u001b[1m\u001b[31mE           NameError: name 'fit_' is not defined\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mdtree.py\u001b[0m:200: NameError\n",
      "\u001b[31m\u001b[1m__________________________ test_wine_min_samples_leaf __________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_wine_min_samples_leaf\u001b[39;49;00m():\n",
      "        X, y = load_wine(return_X_y=\u001b[94mTrue\u001b[39;49;00m)\n",
      ">       run_classification_test(X, y, ntrials=\u001b[94m10\u001b[39;49;00m, min_samples_leaf=\u001b[94m3\u001b[39;49;00m, grace=\u001b[94m0.2\u001b[39;49;00m)\n",
      "\n",
      "\u001b[1m\u001b[31mtest_dtree.py\u001b[0m:30: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtest_dtree.py\u001b[0m:87: in run_classification_test\n",
      "    dt.fit(X_train, y_train)\n",
      "\u001b[1m\u001b[31mdtree.py\u001b[0m:171: in fit\n",
      "    \u001b[96mself\u001b[39;49;00m.root = \u001b[96mself\u001b[39;49;00m.fit_(X, y)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <dtree.ClassifierTree621 object at 0x7fa6e0907c40>\n",
      "X = array([[1.329e+01, 1.970e+00, 2.680e+00, ..., 1.070e+00, 2.840e+00,\n",
      "        1.270e+03],\n",
      "       [1.305e+01, 5.800e+00, ...3.570e+00,\n",
      "        6.720e+02],\n",
      "       [1.220e+01, 3.030e+00, 2.320e+00, ..., 6.600e-01, 1.830e+00,\n",
      "        5.100e+02]])\n",
      "y = array([0, 1, 0, 0, 0, 1, 2, 0, 1, 2, 0, 2, 1, 1, 1, 1, 1, 0, 2, 1, 0, 1,\n",
      "       0, 2, 0, 0, 2, 1, 0, 1, 1, 2, 2, 0, 1,...1, 2, 2,\n",
      "       2, 2, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 1, 1,\n",
      "       2, 1, 1, 1, 1, 0, 2, 1, 1, 2])\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mfit_\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, X, y):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Recursively create and return a decision tree fit to (X,y) for\u001b[39;49;00m\n",
      "    \u001b[33m    either a classification or regression.  This function should call self.create_leaf(X,y)\u001b[39;49;00m\n",
      "    \u001b[33m    to create the appropriate leaf node, which will invoke either\u001b[39;49;00m\n",
      "    \u001b[33m    RegressionTree621.create_leaf() or ClassifierTree621.create_leaf() depending\u001b[39;49;00m\n",
      "    \u001b[33m    on the type of self.\u001b[39;49;00m\n",
      "    \u001b[33m    This function is not part of the class \"interface\" and is for internal use, but it\u001b[39;49;00m\n",
      "    \u001b[33m    embodies the decision tree fitting algorithm.\u001b[39;49;00m\n",
      "    \u001b[33m    (Make sure to call fit_() not fit() recursively.)\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "        ...\n",
      "        \u001b[94mif\u001b[39;49;00m np.shape(X)[\u001b[94m0\u001b[39;49;00m] <= \u001b[96mself\u001b[39;49;00m.min_samples_leaf \u001b[95mor\u001b[39;49;00m np.shape(np.unique(X, axis=\u001b[94m0\u001b[39;49;00m)[\u001b[94m0\u001b[39;49;00m])==\u001b[94m1\u001b[39;49;00m:\n",
      "            \u001b[94mreturn\u001b[39;49;00m create_leaf(y)\n",
      "    \n",
      "        col,split=find_best_split(X,y,\u001b[96mself\u001b[39;49;00m.loss,\u001b[96mself\u001b[39;49;00m.min_samples_leaf)\n",
      "    \n",
      "        \u001b[94mif\u001b[39;49;00m col==-\u001b[94m1\u001b[39;49;00m:\n",
      "            \u001b[94mreturn\u001b[39;49;00m create_leaf(y)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "        \u001b[90m#node=DecisionNode(col,split,fit_(self,X,y)\u001b[39;49;00m\n",
      "            y_reshape=np.reshape(y,(np.shape(y)[\u001b[94m0\u001b[39;49;00m],\u001b[94m1\u001b[39;49;00m)) \u001b[90m#reshaping so that dimensions align\u001b[39;49;00m\n",
      "            Xy=np.hstack((X,y_reshape))\n",
      "    \n",
      "    \n",
      "    \n",
      ">           lchild=fit_(Xy[Xy[col]<split][:,:-\u001b[94m1\u001b[39;49;00m],Xy[Xy[col]<split][:,-\u001b[94m1\u001b[39;49;00m])\n",
      "\u001b[1m\u001b[31mE           NameError: name 'fit_' is not defined\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mdtree.py\u001b[0m:200: NameError\n",
      "\u001b[31m\u001b[1m______________________________ test_breast_cancer ______________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_breast_cancer\u001b[39;49;00m():\n",
      "        X, y = load_breast_cancer(return_X_y=\u001b[94mTrue\u001b[39;49;00m)\n",
      ">       run_classification_test(X, y, ntrials=\u001b[94m5\u001b[39;49;00m, grace=\u001b[94m0.05\u001b[39;49;00m)\n",
      "\n",
      "\u001b[1m\u001b[31mtest_dtree.py\u001b[0m:34: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtest_dtree.py\u001b[0m:87: in run_classification_test\n",
      "    dt.fit(X_train, y_train)\n",
      "\u001b[1m\u001b[31mdtree.py\u001b[0m:171: in fit\n",
      "    \u001b[96mself\u001b[39;49;00m.root = \u001b[96mself\u001b[39;49;00m.fit_(X, y)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <dtree.ClassifierTree621 object at 0x7fa6e090ad90>\n",
      "X = array([[1.561e+01, 1.938e+01, 1.000e+02, ..., 8.568e-02, 2.683e-01,\n",
      "        6.829e-02],\n",
      "       [1.320e+01, 1.582e+01, ...2.894e-01,\n",
      "        7.664e-02],\n",
      "       [1.402e+01, 1.566e+01, 8.959e+01, ..., 8.216e-02, 2.136e-01,\n",
      "        6.710e-02]])\n",
      "y = array([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,...0, 0, 1, 0, 1, 0, 1, 1, 1,\n",
      "       0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "       1, 1, 1, 1])\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mfit_\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, X, y):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Recursively create and return a decision tree fit to (X,y) for\u001b[39;49;00m\n",
      "    \u001b[33m    either a classification or regression.  This function should call self.create_leaf(X,y)\u001b[39;49;00m\n",
      "    \u001b[33m    to create the appropriate leaf node, which will invoke either\u001b[39;49;00m\n",
      "    \u001b[33m    RegressionTree621.create_leaf() or ClassifierTree621.create_leaf() depending\u001b[39;49;00m\n",
      "    \u001b[33m    on the type of self.\u001b[39;49;00m\n",
      "    \u001b[33m    This function is not part of the class \"interface\" and is for internal use, but it\u001b[39;49;00m\n",
      "    \u001b[33m    embodies the decision tree fitting algorithm.\u001b[39;49;00m\n",
      "    \u001b[33m    (Make sure to call fit_() not fit() recursively.)\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "        ...\n",
      "        \u001b[94mif\u001b[39;49;00m np.shape(X)[\u001b[94m0\u001b[39;49;00m] <= \u001b[96mself\u001b[39;49;00m.min_samples_leaf \u001b[95mor\u001b[39;49;00m np.shape(np.unique(X, axis=\u001b[94m0\u001b[39;49;00m)[\u001b[94m0\u001b[39;49;00m])==\u001b[94m1\u001b[39;49;00m:\n",
      "            \u001b[94mreturn\u001b[39;49;00m create_leaf(y)\n",
      "    \n",
      "        col,split=find_best_split(X,y,\u001b[96mself\u001b[39;49;00m.loss,\u001b[96mself\u001b[39;49;00m.min_samples_leaf)\n",
      "    \n",
      "        \u001b[94mif\u001b[39;49;00m col==-\u001b[94m1\u001b[39;49;00m:\n",
      "            \u001b[94mreturn\u001b[39;49;00m create_leaf(y)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "        \u001b[90m#node=DecisionNode(col,split,fit_(self,X,y)\u001b[39;49;00m\n",
      "            y_reshape=np.reshape(y,(np.shape(y)[\u001b[94m0\u001b[39;49;00m],\u001b[94m1\u001b[39;49;00m)) \u001b[90m#reshaping so that dimensions align\u001b[39;49;00m\n",
      "            Xy=np.hstack((X,y_reshape))\n",
      "    \n",
      "    \n",
      "    \n",
      ">           lchild=fit_(Xy[Xy[col]<split][:,:-\u001b[94m1\u001b[39;49;00m],Xy[Xy[col]<split][:,-\u001b[94m1\u001b[39;49;00m])\n",
      "\u001b[1m\u001b[31mE           NameError: name 'fit_' is not defined\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mdtree.py\u001b[0m:200: NameError\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "test_dtree.py::test_boston\n",
      "test_dtree.py::test_boston_min_samples_leaf\n",
      "  /Users/akshaypamnani/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "  \n",
      "      The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "      the documentation of this function for further details.\n",
      "  \n",
      "      The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "      dataset unless the purpose of the code is to study and educate about\n",
      "      ethical issues in data science and machine learning.\n",
      "  \n",
      "      In this special case, you can fetch the dataset from the original\n",
      "      source::\n",
      "  \n",
      "          import pandas as pd\n",
      "          import numpy as np\n",
      "  \n",
      "  \n",
      "          data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "          raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "          data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "          target = raw_df.values[1::2, 2]\n",
      "  \n",
      "      Alternative datasets include the California housing dataset (i.e.\n",
      "      :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "      dataset. You can load the datasets as follows::\n",
      "  \n",
      "          from sklearn.datasets import fetch_california_housing\n",
      "          housing = fetch_california_housing()\n",
      "  \n",
      "      for the California housing dataset and::\n",
      "  \n",
      "          from sklearn.datasets import fetch_openml\n",
      "          housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "  \n",
      "      for the Ames housing dataset.\n",
      "      \n",
      "    warnings.warn(msg, category=FutureWarning)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "=========================== short test summary info ============================\n",
      "FAILED test_dtree.py::test_boston - NameError: name 'fit_' is not defined\n",
      "FAILED test_dtree.py::test_boston_min_samples_leaf - NameError: name 'fit_' i...\n",
      "FAILED test_dtree.py::test_california_housing - NameError: name 'fit_' is not...\n",
      "FAILED test_dtree.py::test_iris - NameError: name 'fit_' is not defined\n",
      "FAILED test_dtree.py::test_wine - NameError: name 'fit_' is not defined\n",
      "FAILED test_dtree.py::test_wine_min_samples_leaf - NameError: name 'fit_' is ...\n",
      "FAILED test_dtree.py::test_breast_cancer - NameError: name 'fit_' is not defined\n",
      "\u001b[31m======================== \u001b[31m\u001b[1m7 failed\u001b[0m, \u001b[33m2 warnings\u001b[0m\u001b[31m in 1.46s\u001b[0m\u001b[31m =========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed43127-1caf-491f-8273-2c3d582e729b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
